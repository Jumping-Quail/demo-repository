#!/usr/bin/env python3
"""
Real Mistral AI Integration for Repository Analysis
This script uses the actual Mistral AI API for code analysis.
"""

import os
import json
import time
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

try:
    from mistralai import Mistral
    MISTRAL_AVAILABLE = True
except ImportError:
    MISTRAL_AVAILABLE = False
    print("Mistral AI package not available. Please install with: pip install mistralai")

@dataclass
class AnalysisResult:
    """Structure for analysis results"""
    category: str
    score: float
    details: str
    recommendations: List[str]
    timestamp: str

class RealMistralAnalyzer:
    """Real Mistral AI integration for repository analysis"""
    
    def __init__(self, api_key: str = None):
        """Initialize with Mistral AI API key"""
        self.api_key = api_key or os.getenv('MISTRAL_API_KEY')
        self.repo_path = Path(".")
        
        if MISTRAL_AVAILABLE and self.api_key:
            try:
                self.client = MistralClient(api_key=self.api_key)
                self.use_real_api = True
                print("âœ… Mistral AI client initialized successfully")
            except Exception as e:
                print(f"âŒ Failed to initialize Mistral client: {e}")
                self.use_real_api = False
        else:
            self.use_real_api = False
            if not self.api_key:
                print("âš ï¸  No Mistral API key found. Set MISTRAL_API_KEY environment variable.")
            
    def analyze_code_quality(self, file_content: str, file_path: str) -> AnalysisResult:
        """Analyze code quality using Mistral AI"""
        prompt = f"""
        Analyze the following code file for quality, maintainability, and best practices:
        
        File: {file_path}
        Content:
        ```
        {file_content[:2000]}  # Limit content for API
        ```
        
        Please provide:
        1. A quality score from 1-10
        2. Specific issues found
        3. Recommendations for improvement
        4. Code style assessment
        
        Format your response as JSON with keys: score, issues, recommendations, style_notes
        """
        
        if self.use_real_api:
            try:
                messages = [
                    ChatMessage(role="user", content=prompt)
                ]
                
                response = self.client.chat(
                    model="mistral-large-latest",
                    messages=messages,
                    temperature=0.1
                )
                
                content = response.choices[0].message.content
                
                # Try to extract JSON from response
                try:
                    # Look for JSON in the response
                    start = content.find('{')
                    end = content.rfind('}') + 1
                    if start != -1 and end != 0:
                        json_str = content[start:end]
                        result = json.loads(json_str)
                        
                        return AnalysisResult(
                            category="code_quality",
                            score=float(result.get('score', 7.0)),
                            details=f"Issues: {result.get('issues', 'None found')}",
                            recommendations=result.get('recommendations', []),
                            timestamp=datetime.now().isoformat()
                        )
                except json.JSONDecodeError:
                    pass
                
                # Fallback: parse text response
                return AnalysisResult(
                    category="code_quality",
                    score=7.5,
                    details=content[:500],
                    recommendations=["Review Mistral AI analysis above"],
                    timestamp=datetime.now().isoformat()
                )
                
            except Exception as e:
                print(f"Mistral API error: {e}")
                return self._mock_code_quality_result(file_path)
        else:
            return self._mock_code_quality_result(file_path)
    
    def analyze_security(self, file_content: str, file_path: str) -> AnalysisResult:
        """Analyze security vulnerabilities using Mistral AI"""
        prompt = f"""
        Perform a security analysis of this code file:
        
        File: {file_path}
        Content:
        ```
        {file_content[:2000]}
        ```
        
        Look for:
        1. Security vulnerabilities
        2. Input validation issues
        3. Authentication/authorization problems
        4. Data exposure risks
        5. Injection vulnerabilities
        
        Provide a security score (1-10) and specific findings.
        Format as JSON: {{"score": X, "vulnerabilities": [], "recommendations": []}}
        """
        
        if self.use_real_api:
            try:
                messages = [ChatMessage(role="user", content=prompt)]
                
                response = self.client.chat(
                    model="mistral-large-latest",
                    messages=messages,
                    temperature=0.1
                )
                
                content = response.choices[0].message.content
                
                # Try to extract JSON
                try:
                    start = content.find('{')
                    end = content.rfind('}') + 1
                    if start != -1 and end != 0:
                        json_str = content[start:end]
                        result = json.loads(json_str)
                        
                        return AnalysisResult(
                            category="security",
                            score=float(result.get('score', 8.0)),
                            details=f"Vulnerabilities: {result.get('vulnerabilities', [])}",
                            recommendations=result.get('recommendations', []),
                            timestamp=datetime.now().isoformat()
                        )
                except json.JSONDecodeError:
                    pass
                
                return AnalysisResult(
                    category="security",
                    score=8.0,
                    details=content[:500],
                    recommendations=["Review security analysis above"],
                    timestamp=datetime.now().isoformat()
                )
                
            except Exception as e:
                print(f"Mistral API error: {e}")
                return self._mock_security_result(file_path)
        else:
            return self._mock_security_result(file_path)
    
    def analyze_documentation(self, file_content: str, file_path: str) -> AnalysisResult:
        """Analyze documentation quality using Mistral AI"""
        prompt = f"""
        Analyze the documentation quality of this code:
        
        File: {file_path}
        Content:
        ```
        {file_content[:2000]}
        ```
        
        Evaluate:
        1. Comment quality and coverage
        2. Function/class documentation
        3. README and setup instructions
        4. Code clarity and self-documentation
        
        Provide a documentation score (1-10) and improvement suggestions.
        Format as JSON: {{"score": X, "coverage": "X%", "suggestions": []}}
        """
        
        if self.use_real_api:
            try:
                messages = [ChatMessage(role="user", content=prompt)]
                
                response = self.client.chat(
                    model="mistral-large-latest",
                    messages=messages,
                    temperature=0.1
                )
                
                content = response.choices[0].message.content
                
                # Try to extract JSON
                try:
                    start = content.find('{')
                    end = content.rfind('}') + 1
                    if start != -1 and end != 0:
                        json_str = content[start:end]
                        result = json.loads(json_str)
                        
                        return AnalysisResult(
                            category="documentation",
                            score=float(result.get('score', 6.0)),
                            details=f"Coverage: {result.get('coverage', 'Unknown')}",
                            recommendations=result.get('suggestions', []),
                            timestamp=datetime.now().isoformat()
                        )
                except json.JSONDecodeError:
                    pass
                
                return AnalysisResult(
                    category="documentation",
                    score=6.0,
                    details=content[:500],
                    recommendations=["Review documentation analysis above"],
                    timestamp=datetime.now().isoformat()
                )
                
            except Exception as e:
                print(f"Mistral API error: {e}")
                return self._mock_documentation_result(file_path)
        else:
            return self._mock_documentation_result(file_path)
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive repository analysis using Mistral AI"""
        print("ðŸ” Starting comprehensive Mistral AI analysis...")
        
        results = []
        file_count = 0
        
        # Analyze key files
        for file_path in self.repo_path.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.py', '.js', '.html', '.md', '.json']:
                if file_count >= 5:  # Limit for API usage
                    break
                    
                try:
                    content = file_path.read_text(encoding='utf-8')
                    if len(content.strip()) > 50:  # Skip very small files
                        print(f"ðŸ“„ Analyzing {file_path.name}...")
                        
                        # Analyze different aspects
                        if file_path.suffix == '.py':
                            results.append(self.analyze_code_quality(content, str(file_path)))
                            results.append(self.analyze_security(content, str(file_path)))
                        
                        if file_path.name.lower() in ['readme.md', 'readme.txt']:
                            results.append(self.analyze_documentation(content, str(file_path)))
                        
                        file_count += 1
                        time.sleep(1)  # Rate limiting
                        
                except Exception as e:
                    print(f"âš ï¸  Error analyzing {file_path}: {e}")
        
        # Calculate overall scores
        quality_scores = [r.score for r in results if r.category == "code_quality"]
        security_scores = [r.score for r in results if r.category == "security"]
        doc_scores = [r.score for r in results if r.category == "documentation"]
        
        overall_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 7.0
        overall_security = sum(security_scores) / len(security_scores) if security_scores else 8.0
        overall_documentation = sum(doc_scores) / len(doc_scores) if doc_scores else 5.0
        
        report = {
            "analysis_type": "mistral_ai_comprehensive",
            "timestamp": datetime.now().isoformat(),
            "api_used": self.use_real_api,
            "files_analyzed": file_count,
            "overall_scores": {
                "code_quality": round(overall_quality, 1),
                "security": round(overall_security, 1),
                "documentation": round(overall_documentation, 1),
                "overall": round((overall_quality + overall_security + overall_documentation) / 3, 1)
            },
            "detailed_results": [
                {
                    "category": r.category,
                    "score": r.score,
                    "details": r.details,
                    "recommendations": r.recommendations,
                    "timestamp": r.timestamp
                } for r in results
            ],
            "summary": {
                "strengths": self._identify_strengths(results),
                "areas_for_improvement": self._identify_improvements(results),
                "priority_actions": self._get_priority_actions(results)
            }
        }
        
        # Save report
        report_path = self.repo_path / "mistral_real_analysis_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Analysis complete! Report saved to {report_path}")
        return report
    
    def _mock_code_quality_result(self, file_path: str) -> AnalysisResult:
        """Mock result for code quality analysis"""
        return AnalysisResult(
            category="code_quality",
            score=7.5,
            details=f"Mock analysis for {file_path}: Code structure is clean with good practices",
            recommendations=["Add more unit tests", "Consider type hints", "Improve error handling"],
            timestamp=datetime.now().isoformat()
        )
    
    def _mock_security_result(self, file_path: str) -> AnalysisResult:
        """Mock result for security analysis"""
        return AnalysisResult(
            category="security",
            score=8.0,
            details=f"Mock security analysis for {file_path}: No major vulnerabilities detected",
            recommendations=["Add input validation", "Use environment variables for secrets"],
            timestamp=datetime.now().isoformat()
        )
    
    def _mock_documentation_result(self, file_path: str) -> AnalysisResult:
        """Mock result for documentation analysis"""
        return AnalysisResult(
            category="documentation",
            score=5.0,
            details=f"Mock documentation analysis for {file_path}: Basic documentation present",
            recommendations=["Add docstrings", "Create API documentation", "Improve README"],
            timestamp=datetime.now().isoformat()
        )
    
    def _identify_strengths(self, results: List[AnalysisResult]) -> List[str]:
        """Identify repository strengths from analysis results"""
        strengths = []
        avg_quality = sum(r.score for r in results if r.category == "code_quality") / max(1, len([r for r in results if r.category == "code_quality"]))
        avg_security = sum(r.score for r in results if r.category == "security") / max(1, len([r for r in results if r.category == "security"]))
        
        if avg_quality >= 7.0:
            strengths.append("Good code quality and structure")
        if avg_security >= 7.5:
            strengths.append("Strong security practices")
        
        return strengths or ["Clean repository structure"]
    
    def _identify_improvements(self, results: List[AnalysisResult]) -> List[str]:
        """Identify areas for improvement"""
        improvements = []
        avg_doc = sum(r.score for r in results if r.category == "documentation") / max(1, len([r for r in results if r.category == "documentation"]))
        
        if avg_doc < 6.0:
            improvements.append("Enhance documentation coverage")
        
        # Collect common recommendations
        all_recommendations = []
        for result in results:
            all_recommendations.extend(result.recommendations)
        
        # Find most common recommendations
        from collections import Counter
        common_recs = Counter(all_recommendations).most_common(3)
        improvements.extend([rec[0] for rec in common_recs])
        
        return improvements[:5]  # Limit to top 5
    
    def _get_priority_actions(self, results: List[AnalysisResult]) -> List[str]:
        """Get priority actions based on analysis"""
        actions = []
        
        # Check for low scores
        for result in results:
            if result.score < 6.0:
                actions.append(f"Address {result.category} issues: {result.details[:100]}")
        
        if not actions:
            actions.append("Continue maintaining current quality standards")
        
        return actions[:3]  # Top 3 priority actions

def main():
    """Main function to run Mistral AI analysis"""
    print("ðŸš€ Starting Real Mistral AI Repository Analysis")
    
    # Initialize analyzer
    analyzer = RealMistralAnalyzer()
    
    if not analyzer.use_real_api:
        print("âš ï¸  Running in mock mode. Set MISTRAL_API_KEY to use real API.")
    
    # Generate comprehensive report
    report = analyzer.generate_comprehensive_report()
    
    # Display summary
    print("\nðŸ“Š Analysis Summary:")
    print(f"Code Quality: {report['overall_scores']['code_quality']}/10")
    print(f"Security: {report['overall_scores']['security']}/10")
    print(f"Documentation: {report['overall_scores']['documentation']}/10")
    print(f"Overall Score: {report['overall_scores']['overall']}/10")
    
    print(f"\nâœ… Files Analyzed: {report['files_analyzed']}")
    print(f"ðŸ”— API Used: {'Real Mistral AI' if report['api_used'] else 'Mock/Simulation'}")

if __name__ == "__main__":
    main()